import json
from langchain_core.messages import SystemMessage, HumanMessage
from ..state.state import SectionState
from ..tools.lightrag_search import LightRAGSearch
from ..prompts.prompts import SYSTEM_PROMPT_FIRST_SEARCH
from ..utils import load_config

config = load_config()
rag_tool = LightRAGSearch()

def search_node(state: SectionState, llm):
    """
    æœç´¢èŠ‚ç‚¹ï¼šæ”¯æŒã€åˆæ¬¡æ„å›¾ç”Ÿæˆã€‘å’Œã€åæ€è¡¥æœã€‘ä¸¤ç§æ¨¡å¼
    """
    query_to_search = ""
    search_reasoning = ""
    
    # --- ä¿®æ”¹ç‚¹ 1ï¼šä½¿ç”¨å­—å…¸æ–¹å¼è®¿é—® section_def ---
    section_def = state["section_def"]
    # ç¡®ä¿ section_def æ˜¯å­—å…¸
    if not isinstance(section_def, dict):
        # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœæ˜¯å¯¹è±¡åˆ™è½¬å­—å…¸ï¼Œæˆ–è€…ç›´æ¥æŠ¥é”™
        try:
            section_def = section_def.__dict__
        except:
            pass

    # A. åæ€åçš„è¡¥æœ
    if state.get("feedback_search_query"):
        query_to_search = state["feedback_search_query"]
        search_reasoning = f"å“åº”åæ€ä¿®æ”¹: {state.get('critique')}"
        print(f"ğŸ” [Search] æ‰§è¡Œè¡¥æœ: {query_to_search}")
        
    # B. åˆæ¬¡æœç´¢
    else:
        print(f"ğŸ” [Search] æ­£åœ¨ç”Ÿæˆåˆæ¬¡æœç´¢è¯...")
        query_to_search, search_reasoning = _generate_initial_query(state, llm)
        print(f"  > ç”ŸæˆæŸ¥è¯¢: {query_to_search}")

    # æ‰§è¡Œæœç´¢
    try:
        results = rag_tool.search(query_to_search, max_results=5)
    except Exception as e:
        print(f"  > [Error] æœç´¢å·¥å…·è°ƒç”¨å¤±è´¥: {e}")
        results = []

    # æ ¼å¼åŒ–ç»“æœ
    new_info = []
    if results:
        print(f"  > è·å¾— {len(results)} æ¡ç»“æœ")
        for res in results:
            snippet = {
                "title": res.get('title', 'æœªçŸ¥æ ‡é¢˜'), # <--- åŠ ä¸Šè¿™ä¸€è¡Œï¼
                "content": f"ã€æ¥æº: {res.get('title', 'æœªçŸ¥')}ã€‘\n{res.get('content', '')}",
                "url": res.get("url", ""),
                "query": query_to_search
            }
            new_info.append(snippet)
    else:
        print("  > âš ï¸ æœªæœç´¢åˆ°æœ‰æ•ˆä¿¡æ¯")

    current_results = state.get("search_results", [])
    updated_results = current_results + new_info
    
    return {
        "search_results": updated_results,
        "feedback_search_query": None
    }

def _generate_initial_query(state: SectionState, llm):
    """
    è¾…åŠ©å‡½æ•°ï¼šè°ƒç”¨ LLM ç”Ÿæˆæœç´¢è¯
    """
    # --- ä¿®æ”¹ç‚¹ 2ï¼šä½¿ç”¨å­—å…¸æ–¹å¼è®¿é—® ---
    section_def = state["section_def"]
    section_title = section_def["title"]   # ä¹‹å‰æ˜¯ .title
    instruction = section_def["content"]   # ä¹‹å‰æ˜¯ .content
    
    input_data = {
        "title": section_title,
        "content": instruction
    }
    
    messages = [
        SystemMessage(content=SYSTEM_PROMPT_FIRST_SEARCH),
        HumanMessage(content=json.dumps(input_data, ensure_ascii=False))
    ]
    
    try:
        response = llm.invoke(messages, response_format={"type": "json_object"})
        result = json.loads(response.content)
        
        query = result.get("search_query", state["query"])
        reasoning = result.get("reasoning", "")
        
        if state["query"] not in query:
            query = f"{state['query']} {query}"
            
        return query, reasoning
        
    except Exception as e:
        print(f"  > [Error] æœç´¢æ„å›¾ç”Ÿæˆå¤±è´¥: {e}")
        fallback = f"{state['query']} {section_title}"
        return fallback, "ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨å…œåº•æŸ¥è¯¢"